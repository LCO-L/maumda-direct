import streamlit as st
import streamlit.components.v1 as components
from services.llm import analyze_text, normalize_data
from services.notion import save_record
from services.voice_input import get_voice_input
from services.auth import check_password, validate_api_usage, log_activity, check_api_limit
import re
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import base64
from PIL import Image
import io

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë§ˆìŒë‹¤ì´ë ‰íŠ¸ ğŸ’¼",
    page_icon="ğŸ—",
    layout="wide"
)

# ============================================
# ğŸ” ë³´ì•ˆ: ë¡œê·¸ì¸ ì²´í¬ (ê°€ì¥ ë¨¼ì €!)
# ============================================
if not check_password():
    st.stop()  # ë¡œê·¸ì¸ ì•ˆ í•˜ë©´ ì—¬ê¸°ì„œ ë©ˆì¶¤

# ============================================
# ë©”ì¸ ì•± ì‹œì‘ (ë¡œê·¸ì¸ ì„±ê³µ í›„)
# ============================================

# API ì‚¬ìš©ëŸ‰ í‘œì‹œ
usage, limits = validate_api_usage()
with st.sidebar:
    st.markdown("### ğŸ“Š ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰")
    st.progress(usage['gpt_calls'] / limits['gpt_calls'] if limits['gpt_calls'] > 0 else 0)
    st.caption(f"AI ë¶„ì„: {usage['gpt_calls']}/{limits['gpt_calls']}")
    st.progress(usage['whisper_calls'] / limits['whisper_calls'] if limits['whisper_calls'] > 0 else 0)
    st.caption(f"ìŒì„±ì¸ì‹: {usage['whisper_calls']}/{limits['whisper_calls']}")
    st.progress(usage['notion_saves'] / limits['notion_saves'] if limits['notion_saves'] > 0 else 0)
    st.caption(f"ì €ì¥: {usage['notion_saves']}/{limits['notion_saves']}")
        
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'analyzed_data' not in st.session_state:
    st.session_state.analyzed_data = None
if 'saved' not in st.session_state:
    st.session_state.saved = False
if 'voice_input' not in st.session_state:
    st.session_state.voice_input = ""

# í—¬í¼ í•¨ìˆ˜ë“¤
def extract_amount(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ê¸ˆì•¡ ì¶”ì¶œ"""
    if not text:
        return None
    
    patterns = [
        r'(\d+)ë§Œ\s*ì›',
        r'(\d+)ë§Œ',
        r'(\d+,\d+)ì›',
        r'(\d+)ì›'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    
    return text

def process_ocr_image(image):
    """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜)"""
    # ì‹¤ì œë¡œëŠ” Google Vision APIë‚˜ AWS Textract ì‚¬ìš©
    # ì—¬ê¸°ì„œëŠ” ë°ëª¨ìš© ì‹œë®¬ë ˆì´ì…˜
    demo_text = """
    í•œì†”ê±´ì„¤ìì¬
    2025-01-23
    
    ì‹œë©˜íŠ¸ 20í¬: 150,000ì›
    íƒ€ì¼ 50ë°•ìŠ¤: 850,000ì›
    
    í•©ê³„: 1,000,000ì›
    """
    return demo_text

def create_payment_chart(data):
    """ì”ê¸ˆ í˜„í™© ì°¨íŠ¸ ìƒì„±"""
    fig = go.Figure()
    
    for index, row in data.iterrows():
        # ì „ì²´ ëŒ€ë¹„ ë°›ì€ ê¸ˆì•¡ ë¹„ìœ¨
        received_pct = (row['ë°›ì€ê¸ˆì•¡'] / row['ê³„ì•½ê¸ˆì•¡']) * 100 if row['ê³„ì•½ê¸ˆì•¡'] > 0 else 0
        remaining_pct = 100 - received_pct
        
        fig.add_trace(go.Bar(
            name='ë°›ì€ ëˆ',
            x=[row['í˜„ì¥ëª…']],
            y=[row['ë°›ì€ê¸ˆì•¡']],
            text=f"{row['ë°›ì€ê¸ˆì•¡']:,}ì›",
            textposition='inside',
            marker_color='#4CAF50'
        ))
        
        fig.add_trace(go.Bar(
            name='ë°›ì„ ëˆ',
            x=[row['í˜„ì¥ëª…']],
            y=[row['ì”ê¸ˆ']],
            text=f"{row['ì”ê¸ˆ']:,}ì›",
            textposition='inside',
            marker_color='#FF9800'
        ))
    
    fig.update_layout(
        barmode='stack',
        height=400,
        title="í˜„ì¥ë³„ ìˆ˜ê¸ˆ í˜„í™©",
        yaxis_title="ê¸ˆì•¡ (ì›)",
        showlegend=True,
        hovermode='x unified'
    )
    
    return fig

# íƒ€ì´í‹€
st.title("ğŸ— ë§ˆìŒë‹¤ì´ë ‰íŠ¸")
st.caption("ê±´ì„¤í˜„ì¥ ì‚¬ì¥ë‹˜ì˜ ë“ ë“ í•œ ë¹„ì¦ˆë‹ˆìŠ¤ íŒŒíŠ¸ë„ˆ")

# íƒ­ êµ¬ì„±
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’° ë¯¸ìˆ˜ê¸ˆ", "ğŸ“¸ ì˜ìˆ˜ì¦", "ğŸ“Š í˜„í™©", "ğŸ’³ ì”ê¸ˆí‘œ"])

with tab1:
    st.subheader("ë°›ì„ ëˆ ê¸°ë¡í•˜ê¸°")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        # ìŒì„± ì…ë ¥ ì„¹ì…˜
        st.markdown("### ğŸ¤ ìŒì„±ìœ¼ë¡œ ì…ë ¥í•˜ê¸°")
        
        # ì‚¬ìš© ì•ˆë‚´
        with st.expander("ğŸ’¡ ìŒì„± ì…ë ¥ ì‚¬ìš©ë²•", expanded=False):
            st.info("""
            **ğŸ™ï¸ ë…¹ìŒ ë°©ë²•:**
            1. ì•„ë˜ **ğŸ”´ ë…¹ìŒ ì‹œì‘** ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
            2. í¸í•˜ê²Œ ë§ì”€í•˜ì„¸ìš” (ì˜ˆ: "ê°•ë‚¨ ì•„íŒŒíŠ¸ íƒ€ì¼ê³µì‚¬ 500ë§Œì› ë‹¤ìŒì£¼")
            3. ë§ì´ ëë‚˜ë©´ **â¹ï¸ ë…¹ìŒ ì¤‘ì§€** ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
            4. **ğŸ¤– AI ì¸ì‹** ë²„íŠ¼ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
            
            **ğŸ’¬ ë§í•˜ê¸° ì˜ˆì‹œ:**
            - "ë¶êµ¬ì²­ ë°©ìˆ˜ ì‘ì—… ëë‚˜ë©´ ì²œë§Œì› ì”ê¸ˆ"
            - "ê¹€ì‚¬ì¥ ì¸í…Œë¦¬ì–´ ì‚¼ë°±ë§Œì› ë‚´ì¼ ê³„ì•½ê¸ˆ"
            - "ì„œì´ˆ ë¹Œë¼ ë¯¸ì¥ ì´ë°±ë§Œì› ë‹¤ìŒì£¼ ìˆ˜ìš”ì¼"
            """)
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if 'is_recording' not in st.session_state:
            st.session_state.is_recording = False
        if 'audio_data' not in st.session_state:
            st.session_state.audio_data = None
        
        # ìŒì„± ë…¹ìŒ ì‹œë„
        try:
            from audio_recorder_streamlit import audio_recorder
            
            # ë…¹ìŒ ë²„íŠ¼ ë¶„ë¦¬
            col_rec1, col_rec2, col_rec3 = st.columns([1, 1, 2])
            
            with col_rec1:
                if not st.session_state.is_recording:
                    if st.button("ğŸ”´ ë…¹ìŒ ì‹œì‘", use_container_width=True, key="start_recording_btn", type="secondary"):
                        st.session_state.is_recording = True
                        st.rerun()
            
            with col_rec2:
                if st.session_state.is_recording:
                    if st.button("â¹ï¸ ë…¹ìŒ ì¤‘ì§€", use_container_width=True, key="stop_recording_btn", type="secondary"):
                        st.session_state.is_recording = False
                        st.rerun()
            
            with col_rec3:
                if st.session_state.is_recording:
                    st.error("ğŸ”´ **ë…¹ìŒ ì¤‘... ë§ì”€í•´ ì£¼ì„¸ìš”!**")
                else:
                    st.success("ğŸ“ ì¤€ë¹„ë¨")
            
            # ì‹¤ì œ ë…¹ìŒ ì»´í¬ë„ŒíŠ¸ (ìˆ¨ê¹€)
            if st.session_state.is_recording:
                with st.container():
                    audio_bytes = audio_recorder(
                        text="ë…¹ìŒ ì¤‘...",
                        recording_color="#FF0000",
                        neutral_color="#4CAF50",
                        icon_name="microphone",
                        icon_size="2x",
                        pause_threshold=30.0,  # 30ì´ˆë¡œ ëŠ˜ë¦¼
                        key="audio_recorder_widget"
                    )
                    
                    if audio_bytes:
                        st.session_state.audio_data = audio_bytes
                        st.session_state.is_recording = False
                        
                        # ğŸ”¥ ìë™ìœ¼ë¡œ AI ì¸ì‹ ì‹œì‘
                        with st.spinner("ğŸ§ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘... (5~10ì´ˆ)"):
                            try:
                                from openai import OpenAI
                                import tempfile
                                import os
                                import time
                                
                                # API ì œí•œ ì²´í¬
                                if check_api_limit("whisper_calls"):
                                    # OpenAI í´ë¼ì´ì–¸íŠ¸
                                    api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
                                    client = OpenAI(api_key=api_key)
                                    
                                    # ì„ì‹œ íŒŒì¼ ì €ì¥
                                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                                        tmp.write(audio_bytes)
                                        tmp_path = tmp.name
                                    
                                    # Whisper API í˜¸ì¶œ
                                    with open(tmp_path, 'rb') as audio_file:
                                        transcript = client.audio.transcriptions.create(
                                            model="whisper-1",
                                            file=audio_file,
                                            language="ko"
                                        )
                                    
                                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                                    os.unlink(tmp_path)
                                    
                                    # ê²°ê³¼ ì €ì¥
                                    st.session_state.recognized_text = transcript.text
                                    st.session_state.voice_text_input = transcript.text
                                    
                                    # ì¸ì‹ ê²°ê³¼ í‘œì‹œ
                                    st.success(f"âœ… ì¸ì‹ ì™„ë£Œ!")
                                    st.info(f"ğŸ“ **ì¸ì‹ëœ í…ìŠ¤íŠ¸:** {transcript.text}")
                                    
                                    # í™œë™ ë¡œê¹…
                                    log_activity("voice_recognition", {"success": True, "text_length": len(transcript.text)})
                                    
                                    # ì˜¤ë””ì˜¤ ë°ì´í„° ì‚­ì œ
                                    st.session_state.audio_data = None
                                    
                                    # 1ì´ˆ í›„ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°˜ì˜
                                    time.sleep(1)
                                    st.rerun()
                                
                            except Exception as e:
                                st.error(f"âŒ ì¸ì‹ ì‹¤íŒ¨: {e}")
                                log_activity("voice_recognition", {"success": False, "error": str(e)})
                                # ì‹¤íŒ¨ì‹œì—ë„ ì˜¤ë””ì˜¤ëŠ” ë³´ê´€
            
            # ë…¹ìŒëœ ì˜¤ë””ì˜¤ê°€ ìˆì§€ë§Œ ì¸ì‹ ì‹¤íŒ¨í•œ ê²½ìš° ìˆ˜ë™ ë²„íŠ¼ ì œê³µ
            if st.session_state.audio_data and not st.session_state.is_recording:
                st.divider()
                st.warning("âš ï¸ ìë™ ì¸ì‹ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                st.audio(st.session_state.audio_data, format="audio/wav")
                
                col_ai1, col_ai2 = st.columns([1, 1])
                
                with col_ai1:
                    if st.button("ğŸ¤– **ë‹¤ì‹œ ì¸ì‹**", type="primary", use_container_width=True, key="retry_recognize_btn"):
                        with st.spinner("ğŸ§ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘..."):
                            try:
                                from openai import OpenAI
                                import tempfile
                                import os
                                import time
                                
                                api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
                                client = OpenAI(api_key=api_key)
                                
                                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                                    tmp.write(st.session_state.audio_data)
                                    tmp_path = tmp.name
                                
                                with open(tmp_path, 'rb') as audio_file:
                                    transcript = client.audio.transcriptions.create(
                                        model="whisper-1",
                                        file=audio_file,
                                        language="ko"
                                    )
                                
                                os.unlink(tmp_path)
                                
                                st.session_state.recognized_text = transcript.text
                                st.session_state.voice_text_input = transcript.text
                                st.success(f"âœ… ì¸ì‹ ì™„ë£Œ: {transcript.text}")
                                st.session_state.audio_data = None
                                time.sleep(1)
                                st.rerun()
                                
                            except Exception as e:
                                st.error(f"ì¸ì‹ ì‹¤íŒ¨: {e}")
                
                with col_ai2:
                    if st.button("ğŸ”„ ë‹¤ì‹œ ë…¹ìŒ", use_container_width=True, key="re_record_btn"):
                        st.session_state.audio_data = None
                        st.session_state.is_recording = False
                        st.rerun()
        
        except ImportError:
            # ëŒ€ì²´ ìŒì„± ì…ë ¥ ë°©ë²• - íŒŒì¼ ì—…ë¡œë“œ
            st.warning("ğŸ¤ ì‹¤ì‹œê°„ ë…¹ìŒ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì—…ë¡œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            
            audio_file = st.file_uploader(
                "ë…¹ìŒëœ ìŒì„± íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
                type=['wav', 'mp3', 'm4a', 'webm', 'ogg'],
                help="ìŠ¤ë§ˆíŠ¸í°ì´ë‚˜ ì»´í“¨í„°ë¡œ ë…¹ìŒí•œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
                key="audio_file_uploader"
            )
            
            if audio_file:
                st.audio(audio_file)
                
                # íŒŒì¼ ì—…ë¡œë“œì‹œ ìë™ ì¸ì‹
                with st.spinner("ğŸ§ ìŒì„± ì¸ì‹ ì¤‘..."):
                    try:
                        from openai import OpenAI
                        import tempfile
                        import time
                        
                        api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
                        client = OpenAI(api_key=api_key)
                        
                        # íŒŒì¼ì„ ë°”ì´íŠ¸ë¡œ ì½ê¸°
                        audio_bytes = audio_file.read()
                        
                        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                        with tempfile.NamedTemporaryFile(suffix=f".{audio_file.name.split('.')[-1]}", delete=False) as tmp:
                            tmp.write(audio_bytes)
                            tmp_path = tmp.name
                        
                        # Whisper API
                        with open(tmp_path, 'rb') as f:
                            transcript = client.audio.transcriptions.create(
                                model="whisper-1",
                                file=f,
                                language="ko"
                            )
                        
                        os.unlink(tmp_path)
                        
                        # ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì…ë ¥ë€ì— ì¶”ê°€
                        st.session_state.recognized_text = transcript.text
                        st.session_state.voice_text_input = transcript.text
                        st.success(f"âœ… ì¸ì‹ ì™„ë£Œ!")
                        st.info(f"ğŸ“ **ì¸ì‹ëœ í…ìŠ¤íŠ¸:** {transcript.text}")
                        time.sleep(1)
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"âŒ ì¸ì‹ ì‹¤íŒ¨: {e}")
                        st.info("ìŒì„± íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        
        # time ëª¨ë“ˆ import ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°)
        import time
        
        # ëŒ€ì²´ ë°©ë²•: audio_recorder_streamlit íŒ¨í‚¤ì§€ ì‚¬ìš©
        try:
            from audio_recorder_streamlit import audio_recorder
            
            st.markdown("#### ë˜ëŠ” ê°„í¸ ë…¹ìŒ")
            
            # ë…¹ìŒ ë²„íŠ¼ ë¶„ë¦¬
            col_rec1, col_rec2, col_rec3 = st.columns([1, 1, 2])
            
            with col_rec1:
                if not st.session_state.is_recording:
                    if st.button("ğŸ”´ ë…¹ìŒ ì‹œì‘", use_container_width=True, key="start_rec"):
                        st.session_state.is_recording = True
                        st.rerun()
            
            with col_rec2:
                if st.session_state.is_recording:
                    if st.button("â¹ï¸ ë…¹ìŒ ì¤‘ì§€", use_container_width=True, key="stop_rec"):
                        st.session_state.is_recording = False
                        st.rerun()
            
            with col_rec3:
                if st.session_state.is_recording:
                    st.error("ğŸ”´ **ë…¹ìŒ ì¤‘... ë§ì”€í•´ ì£¼ì„¸ìš”!**")
                else:
                    st.success("ğŸ“ ì¤€ë¹„ë¨")
            
            # ì‹¤ì œ ë…¹ìŒ ì»´í¬ë„ŒíŠ¸ (ìˆ¨ê¹€)
            if st.session_state.is_recording:
                audio_bytes = audio_recorder(
                    text="",
                    recording_color="#FF0000",
                    neutral_color="#4CAF50",
                    icon_name="microphone",
                    icon_size="1x",
                    pause_threshold=30.0,  # 30ì´ˆë¡œ ëŠ˜ë¦¼
                    key="hidden_recorder"
                )
                
                if audio_bytes:
                    st.session_state.audio_data = audio_bytes
                    st.session_state.is_recording = False
                    st.rerun()
            
            # ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì²˜ë¦¬
            if st.session_state.audio_data:
                st.success("âœ… ë…¹ìŒ ì™„ë£Œ!")
                st.audio(st.session_state.audio_data, format="audio/wav")
                
                col_ai1, col_ai2, col_ai3 = st.columns([2, 2, 1])
                
                with col_ai1:
                    if st.button("ğŸ¤– AI ì¸ì‹", type="primary", use_container_width=True):
                        # ğŸ” API ì œí•œ ì²´í¬
                        if not check_api_limit("whisper_calls"):
                            st.stop()
                        
                        with st.spinner("ğŸ§ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘... (5~10ì´ˆ)"):
                            try:
                                from openai import OpenAI
                                import tempfile
                                import os
                                
                                # OpenAI í´ë¼ì´ì–¸íŠ¸
                                api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
                                client = OpenAI(api_key=api_key)
                                
                                # ì„ì‹œ íŒŒì¼ ì €ì¥
                                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                                    tmp.write(st.session_state.audio_data)
                                    tmp_path = tmp.name
                                
                                # Progress bar ì¶”ê°€
                                progress_bar = st.progress(0)
                                progress_bar.progress(30, text="ìŒì„± íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
                                
                                # Whisper API í˜¸ì¶œ
                                with open(tmp_path, 'rb') as audio_file:
                                    progress_bar.progress(60, text="AI ë¶„ì„ ì¤‘...")
                                    transcript = client.audio.transcriptions.create(
                                        model="whisper-1",
                                        file=audio_file,
                                        language="ko"
                                    )
                                
                                progress_bar.progress(100, text="ì™„ë£Œ!")
                                
                                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                                os.unlink(tmp_path)
                                
                                # ê²°ê³¼ ì €ì¥
                                st.session_state.recognized_text = transcript.text
                                st.session_state.voice_text_input = transcript.text
                                st.success(f"âœ… ì¸ì‹ ì™„ë£Œ: \"{transcript.text}\"")
                                
                                # Progress bar ì œê±°
                                progress_bar.empty()
                                
                                # ğŸ” í™œë™ ë¡œê¹…
                                log_activity("voice_recognition", {"success": True, "text_length": len(transcript.text)})
                                
                                # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°˜ì˜
                                st.rerun()
                                
                            except Exception as e:
                                st.error(f"âŒ ì¸ì‹ ì‹¤íŒ¨: {e}")
                                log_activity("voice_recognition", {"success": False, "error": str(e)})
                
                with col_ai2:
                    if st.button("ğŸ”„ ë‹¤ì‹œ ë…¹ìŒ", use_container_width=True):
                        st.session_state.audio_data = None
                        st.session_state.is_recording = False
                        st.rerun()
        
        except ImportError:
            # ëŒ€ì²´ ìŒì„± ì…ë ¥ ë°©ë²•
            st.divider()
            st.markdown("#### ğŸ“ ìŒì„± íŒŒì¼ ì—…ë¡œë“œ")
            
            audio_file = st.file_uploader(
                "ë…¹ìŒëœ ìŒì„± íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
                type=['wav', 'mp3', 'm4a', 'webm'],
                help="ìŠ¤ë§ˆíŠ¸í°ì´ë‚˜ ì»´í“¨í„°ë¡œ ë…¹ìŒí•œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
            )
            
            if audio_file:
                st.audio(audio_file)
                
                if st.button("ğŸ¤– AI ìŒì„± ì¸ì‹", type="primary"):
                    with st.spinner("ğŸ§ ìŒì„± ì¸ì‹ ì¤‘..."):
                        try:
                            from openai import OpenAI
                            import tempfile
                            
                            api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
                            client = OpenAI(api_key=api_key)
                            
                            # íŒŒì¼ ì €ì¥
                            with tempfile.NamedTemporaryFile(suffix=f".{audio_file.name.split('.')[-1]}", delete=False) as tmp:
                                tmp.write(audio_file.read())
                                tmp_path = tmp.name
                            
                            # Whisper API
                            with open(tmp_path, 'rb') as f:
                                transcript = client.audio.transcriptions.create(
                                    model="whisper-1",
                                    file=f,
                                    language="ko"
                                )
                            
                            os.unlink(tmp_path)
                            
                            st.session_state.recognized_text = transcript.text
                            st.session_state.voice_text_input = transcript.text
                            st.success(f"âœ… ì¸ì‹ ì™„ë£Œ: \"{transcript.text}\"")
                            st.rerun()
                            
                        except Exception as e:
                            st.error(f"ì¸ì‹ ì‹¤íŒ¨: {e}")
        
        # í…ìŠ¤íŠ¸ ì…ë ¥
        st.markdown("### âœï¸ ì§ì ‘ ì…ë ¥í•˜ê¸°")
        
        # ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ìë™ ì…ë ¥
        default_text = ""
        if 'recognized_text' in st.session_state and st.session_state.recognized_text:
            default_text = st.session_state.recognized_text
            st.success(f"ğŸ¤ ì¸ì‹ëœ ë‚´ìš©ì´ ì•„ë˜ì— ìë™ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            # ì¸ì‹ í›„ ì„¸ì…˜ì—ì„œ ì œê±°í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
            del st.session_state.recognized_text
        
        # voice_text_input ì„¸ì…˜ ê°’ í™•ì¸
        if 'voice_text_input' in st.session_state and st.session_state.voice_text_input:
            default_text = st.session_state.voice_text_input
        
        user_input = st.text_area(
            "ê·¸ëƒ¥ í¸í•˜ê²Œ ë§ì”€í•˜ì„¸ìš”",
            value=default_text,
            placeholder="""ì˜ˆì‹œ:
- ê°•ë‚¨ ì•„íŒŒíŠ¸ íƒ€ì¼ê³µì‚¬ 500ë§Œì› ë‹¤ìŒì£¼ ë°›ê¸°ë¡œ í–ˆì–´
- ë¶êµ¬ì²­ ë°©ìˆ˜ ì‘ì—… ëë‚˜ë©´ 1000ë§Œì› ì”ê¸ˆ""",
            height=120,
            key="main_text_input"  # key ë³€ê²½
        )
    
    with col2:
        # ë¹ ë¥¸ ì…ë ¥ í…œí”Œë¦¿
        st.markdown("### ë¹ ë¥¸ ì…ë ¥")
        if st.button("ğŸ“ ê³„ì•½ê¸ˆ", use_container_width=True):
            st.session_state.voice_text_input = "í˜„ì¥ëª… ê³„ì•½ê¸ˆ ê¸ˆì•¡ ì˜¤ëŠ˜ ë°›ìŒ"
            st.rerun()
        
        if st.button("ğŸ’µ ì¤‘ë„ê¸ˆ", use_container_width=True):
            st.session_state.voice_text_input = "í˜„ì¥ëª… ì¤‘ë„ê¸ˆ ê¸ˆì•¡ ë‚ ì§œ ì˜ˆì •"
            st.rerun()
        
        if st.button("ğŸ’° ì”ê¸ˆ", use_container_width=True):
            st.session_state.voice_text_input = "í˜„ì¥ëª… ì”ê¸ˆ ê¸ˆì•¡ ì™„ë£Œì‹œ ë°›ê¸°"
            st.rerun()
    
    # ë¶„ì„ ë²„íŠ¼
    if st.button("ğŸ” ê¸°ë¡í•˜ê¸°", type="primary"):
        if not user_input or not user_input.strip():
            st.warning("ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            # ğŸ” API ì œí•œ ì²´í¬
            if not check_api_limit("gpt_calls"):
                st.stop()
            
            with st.spinner("AIê°€ ë¶„ì„ ì¤‘..."):
                try:
                    # ë¶„ì„ ë° ì •ê·œí™”
                    raw = analyze_text(user_input)
                    normalized = normalize_data(raw)
                    
                    # ì„¸ì…˜ì— ì €ì¥
                    st.session_state.analyzed_data = normalized
                    st.session_state.saved = False
                    
                    # ğŸ” í™œë™ ë¡œê¹…
                    log_activity("text_analysis", {"success": True, "text_length": len(user_input)})
                    
                except Exception as e:
                    st.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    log_activity("text_analysis", {"success": False, "error": str(e)})

    # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
    if st.session_state.analyzed_data:
        st.divider()
        st.subheader("ğŸ“‹ AI ë¶„ì„ ê²°ê³¼")
        
        data = st.session_state.analyzed_data
        
        if data:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # ë¶„ì„ ê²°ê³¼ ì¹´ë“œ
                st.markdown("### ğŸ“‹ ë‚´ìš© ì •ë¦¬")
                
                with st.container():
                    st.markdown(f"""
                    - **ğŸ—ï¸ í˜„ì¥:** {data.get('who', '-')}
                    - **ğŸ“‹ ë‚´ìš©:** {data.get('what', '-')}  
                    - **ğŸ’° ê¸ˆì•¡:** {data.get('how', '-')}
                    - **ğŸ“… ì–¸ì œ:** {data.get('when', '-')}
                    - **ğŸ“ ìœ„ì¹˜:** {data.get('where', '-')}
                    - **â“ ìœ í˜•:** {data.get('why', '-')}
                    """)
                
                # ìˆ˜ì • ê°€ëŠ¥í•œ í•„ë“œë“¤
                with st.expander("âœï¸ ìˆ˜ì •í•˜ê¸°"):
                    data['who'] = st.text_input("í˜„ì¥ëª…", data.get('who', ''))
                    data['what'] = st.text_input("ì‘ì—… ë‚´ìš©", data.get('what', ''))
                    data['when'] = st.text_input("ë‚ ì§œ", data.get('when', ''))
                    data['where'] = st.text_input("ìœ„ì¹˜", data.get('where', ''))
                    data['why'] = st.text_input("ìœ í˜•", data.get('why', ''))
                    data['how'] = st.text_input("ê¸ˆì•¡", data.get('how', ''))
            
            with col2:
                st.markdown("### ğŸ’¾ ì €ì¥")
                
                if not st.session_state.saved:
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        if st.button("ğŸ’¾ í™•ì • ì €ì¥", type="primary", use_container_width=True):
                            # ğŸ” API ì œí•œ ì²´í¬
                            if not check_api_limit("notion_saves"):
                                st.stop()
                                
                            with st.spinner("ì €ì¥ ì¤‘..."):
                                try:
                                    status, msg = save_record(data)
                                    if 200 <= status < 300:
                                        st.success("âœ… ì €ì¥ ì™„ë£Œ!")
                                        st.session_state.saved = True
                                        
                                        # ğŸ” í™œë™ ë¡œê¹…
                                        log_activity("notion_save", {"success": True, "site": data.get('who')})
                                        
                                        # ì„¸ì…˜ ì •ë¦¬
                                        if 'analyzed_data' in st.session_state:
                                            del st.session_state.analyzed_data
                                        if 'recognized_text' in st.session_state:
                                            del st.session_state.recognized_text
                                    else:
                                        st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {msg}")
                                        log_activity("notion_save", {"success": False})
                                except Exception as e:
                                    st.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")
                                    log_activity("notion_save", {"success": False, "error": str(e)})
                    
                    with col2:
                        if st.button("ğŸ—‘ï¸ ì·¨ì†Œ", use_container_width=True):
                            st.session_state.analyzed_data = None
                            st.rerun()
                else:
                    st.success("âœ… ì €ì¥ë¨")
                    if st.button("ğŸ”„ ìƒˆë¡œ ê¸°ë¡", use_container_width=True):
                        st.session_state.analyzed_data = None
                        st.session_state.saved = False
                        if 'recognized_text' in st.session_state:
                            del st.session_state.recognized_text
                        st.rerun()

# Tab 2: ì˜ìˆ˜ì¦ OCR
with tab2:
    st.subheader("ì˜ìˆ˜ì¦ ì´¬ì˜ & ìë™ ì¸ì‹")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ì¹´ë©”ë¼ ì…ë ¥
        uploaded_file = st.camera_input("ì˜ìˆ˜ì¦ ì´¬ì˜ ğŸ“¸")
        
        # ë˜ëŠ” íŒŒì¼ ì—…ë¡œë“œ
        uploaded_image = st.file_uploader(
            "ë˜ëŠ” ì‚¬ì§„ ì„ íƒ",
            type=['png', 'jpg', 'jpeg'],
            help="ì˜ìˆ˜ì¦ ì‚¬ì§„ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        
        image_to_process = uploaded_file or uploaded_image
        
        if image_to_process:
            st.image(image_to_process, caption="ì—…ë¡œë“œëœ ì˜ìˆ˜ì¦")
    
    with col2:
        if image_to_process:
            st.markdown("### ğŸ“‹ ì¸ì‹ ê²°ê³¼")
            
            with st.spinner("ì˜ìˆ˜ì¦ ë¶„ì„ ì¤‘..."):
                # OCR ì²˜ë¦¬ (ì‹¤ì œë¡œëŠ” Google Vision API ì‚¬ìš©)
                extracted_text = process_ocr_image(image_to_process)
                
            # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ í‘œì‹œ
            st.text_area("ì¸ì‹ëœ ë‚´ìš©", extracted_text, height=200)
            
            # ì¹´í…Œê³ ë¦¬ ì„ íƒ
            category = st.selectbox(
                "ë¶„ë¥˜",
                ["ğŸ”¨ ìì¬ë¹„", "ğŸ‘· ì¸ê±´ë¹„", "â›½ ìœ ë¥˜ë¹„", "ğŸš ì‹ëŒ€", "ğŸš— ê¸°íƒ€"]
            )
            
            # í˜„ì¥ ì„ íƒ
            site = st.text_input("í˜„ì¥ëª…", placeholder="ì˜ˆ: ê°•ë‚¨ ì˜¤í”¼ìŠ¤í…”")
            
            if st.button("ğŸ’¾ ì €ì¥í•˜ê¸°", type="primary"):
                # ğŸ” API ì œí•œ ì²´í¬
                if not check_api_limit("notion_saves"):
                    st.stop()
                    
                # LLMìœ¼ë¡œ ì˜ìˆ˜ì¦ í…ìŠ¤íŠ¸ êµ¬ì¡°í™”
                with st.spinner("ì €ì¥ ì¤‘..."):
                    try:
                        # ì˜ìˆ˜ì¦ í…ìŠ¤íŠ¸ë¥¼ 5W1Hë¡œ ë³€í™˜
                        receipt_input = f"{site} {category} {extracted_text}"
                        raw = analyze_text(receipt_input)
                        normalized = normalize_data(raw)
                        status, msg = save_record(normalized)
                        
                        if 200 <= status < 300:
                            st.success(f"âœ… '{category}' ì˜ìˆ˜ì¦ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            # ğŸ” í™œë™ ë¡œê¹…
                            log_activity("receipt_save", {"success": True, "category": category})
                        else:
                            st.error("ì €ì¥ ì‹¤íŒ¨")
                            log_activity("receipt_save", {"success": False})
                    except Exception as e:
                        st.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        log_activity("receipt_save", {"success": False, "error": str(e)})

# Tab 3: í˜„í™© ëŒ€ì‹œë³´ë“œ
with tab3:
    st.subheader("ì´ë²ˆ ë‹¬ í˜„í™©")
    
    # ë©”íŠ¸ë¦­ ì¹´ë“œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="ì´ ê³„ì•½ê¸ˆì•¡",
            value="8,500ë§Œì›",
            delta="ì‹ ê·œ 500ë§Œì›"
        )
    
    with col2:
        st.metric(
            label="ë°›ì€ ëˆ",
            value="5,250ë§Œì›",
            delta="ì´ë²ˆì£¼ +500ë§Œì›"
        )
    
    with col3:
        st.metric(
            label="ë°›ì„ ëˆ",
            value="3,250ë§Œì›",
            delta="38.2%"
        )
    
    with col4:
        st.metric(
            label="ì§€ì¶œ",
            value="2,130ë§Œì›",
            delta="-230ë§Œì›"
        )
    
    st.divider()
    
    # ë¯¸ìˆ˜ê¸ˆ ì•Œë¦¼
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“Œ ì´ë²ˆ ì£¼ ë°›ì„ ëˆ")
        
        # ë¯¸ìˆ˜ê¸ˆ ë°ì´í„°
        receivables_df = pd.DataFrame([
            {"í˜„ì¥": "ê°•ë‚¨ ì˜¤í”¼ìŠ¤í…”", "êµ¬ë¶„": "ì¤‘ë„ê¸ˆ", "ê¸ˆì•¡": 5000000, "ì˜ˆì •ì¼": "2025-01-25", "D-Day": 2},
            {"í˜„ì¥": "ë¶êµ¬ì²­ ë°©ìˆ˜", "êµ¬ë¶„": "ì”ê¸ˆ", "ê¸ˆì•¡": 10000000, "ì˜ˆì •ì¼": "2025-01-28", "D-Day": 5},
            {"í˜„ì¥": "ì„œì´ˆ ì•„íŒŒíŠ¸", "êµ¬ë¶„": "ê³„ì•½ê¸ˆ", "ê¸ˆì•¡": 3000000, "ì˜ˆì •ì¼": "2025-01-23", "D-Day": 0},
            {"í˜„ì¥": "íŒêµ ë¹Œë¼", "êµ¬ë¶„": "ì¤‘ë„ê¸ˆ", "ê¸ˆì•¡": 4500000, "ì˜ˆì •ì¼": "2025-01-30", "D-Day": 7},
        ])
        
        for _, row in receivables_df.iterrows():
            col_a, col_b, col_c, col_d, col_e = st.columns([3, 2, 2, 1, 1])
            
            with col_a:
                st.write(f"**{row['í˜„ì¥']}**")
            with col_b:
                st.write(f"{row['êµ¬ë¶„']}")
            with col_c:
                st.write(f"{row['ê¸ˆì•¡']:,}ì›")
            with col_d:
                if row['D-Day'] == 0:
                    st.write("ğŸ”´ ì˜¤ëŠ˜")
                elif row['D-Day'] <= 2:
                    st.write(f"ğŸŸ¡ D-{row['D-Day']}")
                else:
                    st.write(f"D-{row['D-Day']}")
            with col_e:
                if st.button("ğŸ“", key=f"call_{row['í˜„ì¥']}"):
                    st.info(f"{row['í˜„ì¥']} ë‹´ë‹¹ì ì—°ê²°")
    
    with col2:
        # ìˆ˜ê¸ˆë¥  íŒŒì´ ì°¨íŠ¸
        fig = go.Figure(data=[go.Pie(
            labels=['ë°›ì€ ëˆ', 'ë°›ì„ ëˆ'],
            values=[5250, 3250],
            hole=.3,
            marker_colors=['#4CAF50', '#FFC107']
        )])
        
        fig.update_layout(
            title="ìˆ˜ê¸ˆ í˜„í™©",
            height=300,
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)

# Tab 4: ì”ê¸ˆ í˜„í™©í‘œ
with tab4:
    st.subheader("ğŸ’³ í˜„ì¥ë³„ ì”ê¸ˆ í˜„í™©")
    
    # ìƒ˜í”Œ ë°ì´í„°
    payment_data = pd.DataFrame([
        {"í˜„ì¥ëª…": "ê°•ë‚¨ ì˜¤í”¼ìŠ¤í…”", "ê³„ì•½ê¸ˆì•¡": 15000000, "ë°›ì€ê¸ˆì•¡": 10000000, "ì”ê¸ˆ": 5000000, "ì§„í–‰ë¥ ": 67},
        {"í˜„ì¥ëª…": "ë¶êµ¬ì²­ ë°©ìˆ˜", "ê³„ì•½ê¸ˆì•¡": 30000000, "ë°›ì€ê¸ˆì•¡": 20000000, "ì”ê¸ˆ": 10000000, "ì§„í–‰ë¥ ": 67},
        {"í˜„ì¥ëª…": "ì„œì´ˆ ì•„íŒŒíŠ¸", "ê³„ì•½ê¸ˆì•¡": 8000000, "ë°›ì€ê¸ˆì•¡": 5000000, "ì”ê¸ˆ": 3000000, "ì§„í–‰ë¥ ": 63},
        {"í˜„ì¥ëª…": "íŒêµ ë¹Œë¼", "ê³„ì•½ê¸ˆì•¡": 12000000, "ë°›ì€ê¸ˆì•¡": 7500000, "ì”ê¸ˆ": 4500000, "ì§„í–‰ë¥ ": 63},
        {"í˜„ì¥ëª…": "ë¶„ë‹¹ ì£¼íƒ", "ê³„ì•½ê¸ˆì•¡": 20000000, "ë°›ì€ê¸ˆì•¡": 20000000, "ì”ê¸ˆ": 0, "ì§„í–‰ë¥ ": 100},
    ])
    
    # ì°¨íŠ¸ í‘œì‹œ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # ë§‰ëŒ€ ì°¨íŠ¸
        fig = create_payment_chart(payment_data)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # ìš”ì•½ ì •ë³´
        st.metric("ì´ ê³„ì•½ê¸ˆì•¡", f"{payment_data['ê³„ì•½ê¸ˆì•¡'].sum():,}ì›")
        st.metric("ì´ ë°›ì€ê¸ˆì•¡", f"{payment_data['ë°›ì€ê¸ˆì•¡'].sum():,}ì›")
        st.metric("ì´ ì”ê¸ˆ", f"{payment_data['ì”ê¸ˆ'].sum():,}ì›")
        
        avg_progress = payment_data['ì§„í–‰ë¥ '].mean()
        st.metric("í‰ê·  ìˆ˜ê¸ˆë¥ ", f"{avg_progress:.1f}%")
    
    # ìƒì„¸ í…Œì´ë¸”
    st.divider()
    st.markdown("### ğŸ“‹ ìƒì„¸ ë‚´ì—­")
    
    # í…Œì´ë¸” ìŠ¤íƒ€ì¼ë§
    styled_df = payment_data.copy()
    styled_df['ê³„ì•½ê¸ˆì•¡'] = styled_df['ê³„ì•½ê¸ˆì•¡'].apply(lambda x: f"{x:,}ì›")
    styled_df['ë°›ì€ê¸ˆì•¡'] = styled_df['ë°›ì€ê¸ˆì•¡'].apply(lambda x: f"{x:,}ì›")
    styled_df['ì”ê¸ˆ'] = styled_df['ì”ê¸ˆ'].apply(lambda x: f"{x:,}ì›")
    styled_df['ì§„í–‰ë¥ '] = styled_df['ì§„í–‰ë¥ '].apply(lambda x: f"{x}%")
    
    # í¸ì§‘ ê°€ëŠ¥í•œ í…Œì´ë¸”
    edited_df = st.data_editor(
        styled_df,
        hide_index=True,
        use_container_width=True,
        column_config={
            "í˜„ì¥ëª…": st.column_config.TextColumn("í˜„ì¥ëª…", width="medium"),
            "ê³„ì•½ê¸ˆì•¡": st.column_config.TextColumn("ê³„ì•½ê¸ˆì•¡", width="small"),
            "ë°›ì€ê¸ˆì•¡": st.column_config.TextColumn("ë°›ì€ê¸ˆì•¡", width="small"),
            "ì”ê¸ˆ": st.column_config.TextColumn("ì”ê¸ˆ", width="small"),
            "ì§„í–‰ë¥ ": st.column_config.ProgressColumn(
                "ì§„í–‰ë¥ ",
                help="ìˆ˜ê¸ˆ ì§„í–‰ë¥ ",
                format="%d%%",
                min_value=0,
                max_value=100,
            ),
        }
    )
    
    # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        if st.button("ğŸ“Š ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", use_container_width=True):
            # ì—‘ì…€ íŒŒì¼ ìƒì„± (ì‹¤ì œë¡œëŠ” pandas to_excel ì‚¬ìš©)
            st.success("ìˆ˜ê¸ˆí˜„í™©.xlsx ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    
    with col2:
        if st.button("ğŸ“¨ ì„¸ë¬´ì‚¬ ì „ì†¡", use_container_width=True):
            st.success("ì„¸ë¬´ì‚¬ë‹˜ê»˜ ìë£Œ ì „ì†¡ ì™„ë£Œ!")

# ============================================
# í•˜ë‹¨ ìƒíƒœë°”
# ============================================
st.divider()

# í•œ ì¤„ë¡œ ëª¨ë“  ì •ë³´ í‘œì‹œ
footer_cols = st.columns([4, 1, 1, 1])

with footer_cols[0]:
    # ì„¸ì…˜ ì •ë³´
    if st.session_state.get('authenticated'):
        from datetime import datetime, timedelta
        
        # ì‚¬ìš©ì ë° ì‹œê°„ ì •ë³´
        user = st.session_state.get('username', 'guest')
        
        # ë‚¨ì€ ì‹œê°„ ê³„ì‚°
        remaining_minutes = 30
        if st.session_state.get('login_time'):
            elapsed = (datetime.now() - st.session_state.login_time).seconds // 60
            remaining_minutes = max(0, 30 - elapsed)
        
        # ì‚¬ìš©ëŸ‰ ì •ë³´
        usage, limits = validate_api_usage()
        
        st.caption(
            f"ğŸ‘¤ {user} | "
            f"â±ï¸ {remaining_minutes}ë¶„ | "
            f"ğŸ“Š AI {usage['gpt_calls']}/{limits['gpt_calls']} | "
            f"ğŸ¤ ìŒì„± {usage['whisper_calls']}/{limits['whisper_calls']}"
        )

with footer_cols[1]:
    st.button("ğŸ“ ì§€ì›", use_container_width=True)

with footer_cols[2]:
    st.button("âš™ï¸ ì„¤ì •", use_container_width=True)

with footer_cols[3]:
    if st.button("ğŸšª ë¡œê·¸ì•„ì›ƒ", use_container_width=True, key="logout_btn_main"):
        st.session_state.clear()
        st.rerun()
